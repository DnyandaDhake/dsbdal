{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da4f320c-8362-475f-9b20-5edc76ed86d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lila': 1, 'girl': 1, 'in': 1, 'and': 1, 'pune': 1, 'she': 1, 'is': 2, 'old': 0, 'studing': 1, 'good': 1, 'year': 0, 'engineerg': 1, '20': 0}\n",
      "{'lila': 1, 'girl': 0, 'in': 0, 'and': 0, 'pune': 0, 'she': 0, 'is': 1, 'old': 1, 'studing': 0, 'good': 0, 'year': 1, 'engineerg': 0, '20': 1}\n",
      "{'lila': 0.09090909090909091, 'girl': 0.09090909090909091, 'in': 0.09090909090909091, 'and': 0.09090909090909091, 'pune': 0.09090909090909091, 'she': 0.09090909090909091, 'is': 0.18181818181818182, 'old': 0.0, 'studing': 0.09090909090909091, 'good': 0.09090909090909091, 'year': 0.0, 'engineerg': 0.09090909090909091, '20': 0.0}\n",
      "{'lila': 0.2, 'girl': 0.0, 'in': 0.0, 'and': 0.0, 'pune': 0.0, 'she': 0.0, 'is': 0.2, 'old': 0.2, 'studing': 0.0, 'good': 0.0, 'year': 0.2, 'engineerg': 0.0, '20': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "word_tokenizatuion ['Hppiness', 'is', 'not', 'ready', 'made', '.', 'It', 'cultivated', 'by', 'the', 'choices', 'we', 'make', ',', 'kindness', 'we', 'share', 'with', 'others', 'and', 'inner', 'self', '.']\n",
      "sentence tokenizatio is  ['Hppiness is not ready made.', 'It cultivated by the choices we make,kindness we share with others and inner self.']\n",
      "cleaned versio is  ['Hppiness', 'ready', 'made', '.', 'cultivated', 'choices', 'make', ',', 'kindness', 'share', 'others', 'inner', 'self', '.']\n",
      "root word for writing is write\n",
      "root word for writes is write\n",
      "root word for gone is gone\n",
      "root word for went is went\n",
      "root word for studying is studi\n",
      "\n",
      "\n",
      "lemma for  writing is write\n",
      "lemma for  writes is write\n",
      "lemma for  gone is go\n",
      "lemma for  went is go\n",
      "lemma for  studying is study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "doc_a=\"lila is good girl and she is studing engineerg in pune\"\n",
    "doc_b=\"lila is 20 year old\"\n",
    "bag_a=doc_a.split(\" \")\n",
    "bag_b=doc_b.split(\" \")\n",
    "uniquewords=set(bag_a).union(set(bag_b))\n",
    "numwordsA=dict.fromkeys(uniquewords,0)\n",
    "for word in bag_a:\n",
    "    numwordsA[word]+=1\n",
    "numwordsB=dict.fromkeys(uniquewords,0)\n",
    "for word in bag_b:\n",
    "    numwordsB[word]+=1\n",
    "\n",
    "print(numwordsA)\n",
    "print(numwordsB)\n",
    "def computetf(worddict,bagwords):\n",
    "    temp={}\n",
    "    bagcount=len(bagwords)\n",
    "    for word,count in worddict.items():\n",
    "        temp[word]=count/float(bagcount)\n",
    "    return temp\n",
    "tfa=computetf(numwordsA,bag_a)\n",
    "tfb=computetf(numwordsB,bag_b)\n",
    "print(tfa)\n",
    "print(tfb)\n",
    "\n",
    "\n",
    "def computeidf(documents):\n",
    "    N=len(documents)\n",
    "    idfdict={}\n",
    "    idfdict=dict.fromkeys(documents[0].keys(),0)\n",
    "    for doc in documents:\n",
    "        for word,val in doc.items():\n",
    "            if val>0:\n",
    "              idfdict[word]+=1\n",
    "\n",
    "    for word,val in idfdict.items():\n",
    "            idfdict[word]=N/float(val)\n",
    "    return idfdict\n",
    "idf1=computeidf([numwordsA,numwordsB])\n",
    "idf1\n",
    "\n",
    "\n",
    "\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "sentence=\"Hppiness is not ready made. It cultivated by the choices we make,kindness we share with others and inner self.\"\n",
    "words=word_tokenize(sentence)\n",
    "print(\"word_tokenizatuion\",words)\n",
    "print(\"sentence tokenizatio is \",sent_tokenize(sentence))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "cleaned=[word for word in words  if word.lower() not in stop_words]\n",
    "print(\"cleaned versio is \", cleaned)\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "ewords=['writing','writes','gone','went','studying']\n",
    "for w in ewords:\n",
    "    root=ps.stem(w)\n",
    "    print(\"root word for\",w,\"is\",root)\n",
    "\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "print(\"\\n\")\n",
    "for w in ewords:\n",
    "    root=lemma.lemmatize(w,pos='v')\n",
    "    print(\"lemma for \",w,\"is\",root)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e850a4a-4ecc-47f6-8f8e-34b80521cbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7035a3e5-01a6-477d-a317-cff1a5189e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: C:\\Program Files\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!\"{sys.executable}\" -m pip install nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e70a5924-c161-4cd2-8da7-f625918539e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26ba0739-b450-4d08-b7df-1924691e4732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenizatuion ['Hppiness', 'is', 'not', 'ready', 'made', '.', 'It', 'cultivated', 'by', 'the', 'choices', 'we', 'make', ',', 'kindness', 'we', 'share', 'with', 'others', 'and', 'inner', 'self', '.']\n",
      "sentence tokenizatio is  ['Hppiness is not ready made.', 'It cultivated by the choices we make,kindness we share with others and inner self.']\n",
      "cleaned versio is  ['Hppiness', 'ready', 'made', '.', 'cultivated', 'choices', 'make', ',', 'kindness', 'share', 'others', 'inner', 'self', '.']\n",
      "root word for writing is write\n",
      "root word for writes is write\n",
      "root word for gone is gone\n",
      "root word for went is went\n",
      "root word for studying is studi\n",
      "\n",
      "\n",
      "lemma for  writing is write\n",
      "lemma for  writes is write\n",
      "lemma for  gone is go\n",
      "lemma for  went is go\n",
      "lemma for  studying is study\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "sentence=\"Hppiness is not ready made. It cultivated by the choices we make,kindness we share with others and inner self.\"\n",
    "words=word_tokenize(sentence)\n",
    "print(\"word_tokenizatuion\",words)\n",
    "print(\"sentence tokenizatio is \",sent_tokenize(sentence))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "cleaned=[word for word in words  if word.lower() not in stop_words]\n",
    "print(\"cleaned versio is \", cleaned)\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "ewords=['writing','writes','gone','went','studying']\n",
    "for w in ewords:\n",
    "    root=ps.stem(w)\n",
    "    print(\"root word for\",w,\"is\",root)\n",
    "\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "print(\"\\n\")\n",
    "for w in ewords:\n",
    "    root=lemma.lemmatize(w,pos='v')\n",
    "    print(\"lemma for \",w,\"is\",root)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4edfec-82a1-42d5-ba7a-5d376b5b2e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
